{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWoHF3eJXeWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "745b725a-735e-44dc-843e-34d28cb8bfde"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMXmQWEdr54Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2095e61f-3e55-4f6e-8c75-5298ad3a9257"
      },
      "source": [
        "!pip install biopython"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.6/dist-packages (1.77)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from biopython) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjrlYH4iXDxC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfa227f7-350c-4933-e877-f9b565610f40"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from Bio.SeqIO import parse\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio.Seq import Seq\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import keras\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, LeakyReLU\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from collections import Counter\n",
        "from sklearn.utils import class_weight\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjQJiovXXDxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/USA_removed.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz7A9VZCXDxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_array(my_string):\n",
        "    my_string = my_string.lower()\n",
        "    my_string = re.sub('[^acgt]', 'z', my_string)\n",
        "    my_array = np.array(list(my_string))\n",
        "    return my_array"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otoHagIgXDxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bff81b10-ee6f-419e-ee91-dbbdddeb3279"
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(np.array(['a','c','g','t','z']))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH8w1VSRXDxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ordinal_encoder(my_array):\n",
        "    \n",
        "    integer_encoded = label_encoder.transform(my_array)\n",
        "    float_encoded = integer_encoded.astype(float)\n",
        "    float_encoded[float_encoded == 0] = 0.25 # A\n",
        "    float_encoded[float_encoded == 1] = 0.50 # C\n",
        "    float_encoded[float_encoded == 2] = 0.75 # G\n",
        "    float_encoded[float_encoded == 3] = 1.00 # T\n",
        "    float_encoded[float_encoded == 4] = 0.00 # anything else, z\n",
        "    \n",
        "    return float_encoded"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhb-cZleXDxs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "8226e7e6-6e34-4447-bf6f-8cba0576d0d6"
      },
      "source": [
        "data=df[[\"Sequence\",\"Geo_Location\"]]\n",
        "data=data[data[\"Sequence\"].notna()]\n",
        "\n",
        "dummy=[]\n",
        "dum=np.array(dummy)\n",
        "\n",
        "form={\"inp_seq\":dum}\n",
        "\n",
        "seq_df = pd.DataFrame (form, columns = ['inp_seq'])\n",
        "\n",
        "seq_list=[]\n",
        "\n",
        "for idx, seq in enumerate(list(data[\"Sequence\"])):\n",
        "    arr=ordinal_encoder(string_to_array(seq))\n",
        "    seq_list.append(arr)\n",
        "    \n",
        "seq_df[\"inp_seq\"]=seq_list\n",
        "\n",
        "final_data= data.assign(enc_seq=seq_df)\n",
        "\n",
        "final_data=final_data[[\"enc_seq\",\"Geo_Location\"]]\n",
        "\n",
        "final_data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>enc_seq</th>\n",
              "      <th>Geo_Location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.5, 0.5, 0.25, 0.25, 0.5, 1.0, 1.0, 1.0, 0.5...</td>\n",
              "      <td>Virginia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1.0, 0.5, 0.5, 0.5, 0.25, 0.75, 0.75, 1.0, 0....</td>\n",
              "      <td>Virginia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.25, 0.5, 0.5, 0.25, 0.25, 0.5, 0.5, 0.25, 0...</td>\n",
              "      <td>Virginia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.25, 0.5, 1.0, 1.0, 1.0, 0.5, 0.75, 0.25, 1....</td>\n",
              "      <td>Virginia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.25, 0.25, 0.25, 0.75, 0.75, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>Virginia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4479</th>\n",
              "      <td>[0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...</td>\n",
              "      <td>California</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4480</th>\n",
              "      <td>[0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...</td>\n",
              "      <td>Washington</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4481</th>\n",
              "      <td>[0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...</td>\n",
              "      <td>Washington</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4482</th>\n",
              "      <td>[0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...</td>\n",
              "      <td>California</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4483</th>\n",
              "      <td>[0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...</td>\n",
              "      <td>California</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4484 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                enc_seq Geo_Location\n",
              "0     [0.5, 0.5, 0.25, 0.25, 0.5, 1.0, 1.0, 1.0, 0.5...     Virginia\n",
              "1     [1.0, 0.5, 0.5, 0.5, 0.25, 0.75, 0.75, 1.0, 0....     Virginia\n",
              "2     [0.25, 0.5, 0.5, 0.25, 0.25, 0.5, 0.5, 0.25, 0...     Virginia\n",
              "3     [0.25, 0.5, 1.0, 1.0, 1.0, 0.5, 0.75, 0.25, 1....     Virginia\n",
              "4     [0.25, 0.25, 0.25, 0.75, 0.75, 1.0, 1.0, 1.0, ...     Virginia\n",
              "...                                                 ...          ...\n",
              "4479  [0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...   California\n",
              "4480  [0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...   Washington\n",
              "4481  [0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...   Washington\n",
              "4482  [0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...   California\n",
              "4483  [0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...   California\n",
              "\n",
              "[4484 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8tt8sE-XDxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_maxLen(enc_seq):\n",
        "    \n",
        "    max=0\n",
        "    for row in enc_seq:\n",
        "        #print(type(row))\n",
        "        if(len(row)>max):\n",
        "            max=len(row)\n",
        "    \n",
        "    \n",
        "    return max"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrjHM7nUXDx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def append_arr(enc_seq, max_len):\n",
        "    \n",
        "    seq_l=list(enc_seq)\n",
        "    for i in range(len(seq_l),max_len):\n",
        "        seq_l.append(0)\n",
        "        \n",
        "    new_seq_ar=np.array(seq_l)\n",
        "\n",
        "        \n",
        "    return new_seq_ar"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br8ums-2XDx7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "a01e1f38-44bb-46d4-df08-ccf1f241ee14"
      },
      "source": [
        "import math\n",
        "\n",
        "max_len=get_maxLen(final_data[\"enc_seq\"])\n",
        "print(\"max_len is\",max_len)\n",
        "if max_len%2 !=0:\n",
        "    max_len += 1\n",
        "\n",
        "n = 1\n",
        "for i in range(1, math.floor(math.sqrt(max_len))):\n",
        "    if max_len%i==0:\n",
        "        n = i\n",
        "\n",
        "padded_seq_list=[]\n",
        "\n",
        "for index, row in final_data.iterrows():\n",
        "    seq_ar=append_arr(row[\"enc_seq\"],max_len)\n",
        "    padded_seq_list.append(seq_ar)\n",
        "\n",
        "dummy=[]\n",
        "dum=np.array(dummy)\n",
        "form={\"padded_enc_seq\":dum}\n",
        "padded_seq_df = pd.DataFrame (form, columns = ['padded_enc_seq'])\n",
        "\n",
        "padded_seq_df[\"padded_enc_seq\"]=padded_seq_list\n",
        "\n",
        "padded_final_data= final_data.assign(padded_enc_seq=padded_seq_df)\n",
        "\n",
        "padded_final_data=padded_final_data[[\"padded_enc_seq\",\"Geo_Location\"]]\n",
        "\n",
        "padded_final_data.head(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_len is 29921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>padded_enc_seq</th>\n",
              "      <th>Geo_Location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.5, 0.5, 0.25, 0.25, 0.5, 1.0, 1.0, 1.0, 0.5...</td>\n",
              "      <td>Virginia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1.0, 0.5, 0.5, 0.5, 0.25, 0.75, 0.75, 1.0, 0....</td>\n",
              "      <td>Virginia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.25, 0.5, 0.5, 0.25, 0.25, 0.5, 0.5, 0.25, 0...</td>\n",
              "      <td>Virginia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.25, 0.5, 1.0, 1.0, 1.0, 0.5, 0.75, 0.25, 1....</td>\n",
              "      <td>Virginia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.25, 0.25, 0.25, 0.75, 0.75, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>Virginia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      padded_enc_seq Geo_Location\n",
              "0  [0.5, 0.5, 0.25, 0.25, 0.5, 1.0, 1.0, 1.0, 0.5...     Virginia\n",
              "1  [1.0, 0.5, 0.5, 0.5, 0.25, 0.75, 0.75, 1.0, 0....     Virginia\n",
              "2  [0.25, 0.5, 0.5, 0.25, 0.25, 0.5, 0.5, 0.25, 0...     Virginia\n",
              "3  [0.25, 0.5, 1.0, 1.0, 1.0, 0.5, 0.75, 0.25, 1....     Virginia\n",
              "4  [0.25, 0.25, 0.25, 0.75, 0.75, 1.0, 1.0, 1.0, ...     Virginia"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTnqbUsqXDyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reshape_seq(seq, m ,n):\n",
        "    \n",
        "    r_seq=np.reshape(seq,(m,n))\n",
        "    \n",
        "    return r_seq\n",
        "\n",
        "re_seqList=[]\n",
        "\n",
        "for index, row in padded_final_data.iterrows():\n",
        "    seq_ar=reshape_seq(row[\"padded_enc_seq\"],n,max_len//n)\n",
        "    re_seqList.append(seq_ar)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfTmmQqXXDyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10bbccf6-f916-4762-932a-98dceda85661"
      },
      "source": [
        "X = np.asarray(re_seqList)\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1))\n",
        "X.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4484, 6, 4987, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVdMOloCXDyM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5c5af82-3aeb-481c-c0fc-04e77e9d6645"
      },
      "source": [
        "Y = padded_final_data['Geo_Location'].values\n",
        "Y.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4484,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPGwIk5SXDyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loc_classes = list(Y)\n",
        "loc_classes = np.array(loc_classes) \n",
        "loc_classes = list(np.unique(loc_classes))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGR_1qKlXDyX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5d43778-4896-4410-8e24-d3b1cd1deb23"
      },
      "source": [
        "df = padded_final_data['Geo_Location'].apply(loc_classes.index)\n",
        "Y = df.values\n",
        "Y.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4484,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5QHcwrgthc4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "eac75a2e-e5ea-480d-fd2e-a74cbc652c33"
      },
      "source": [
        "!pip install keras-metrics"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-metrics in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras-metrics) (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GGn1MbYXDyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import keras_metrics\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, LeakyReLU\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from collections import Counter\n",
        "from sklearn.utils import class_weight\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "import tensorflow as tf"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVA5EwfHXDyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.30)\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_folds = 3 # there will be 3 folds in it's entirety\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ovoA3TUXDyl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5578217-7eae-4800-9077-8ae6692d1173"
      },
      "source": [
        "# print(\"Train X Shape:\",x_train.shape)\n",
        "# print(\"Train Y Shape:\",y_train.shape)\n",
        "# print(\"Validation X Shape:\",x_val.shape)\n",
        "# print(\"Validation Y Shape:\",y_val.shape)\n",
        "print(X.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4484, 6, 4987, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWGFPxAsXDyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch_size = 64\n",
        "# num_classes = 8\n",
        "# epochs = 10\n",
        "\n",
        "#img_rows, img_cols = x_train.shape[1], x_train.shape[2]\n",
        "img_rows, img_cols = X.shape[1],X.shape[2]\n",
        "# x_train = x_train.reshape(x_train.shape[0],img_rows,img_cols,1)\n",
        "# x_val = x_val.reshape(x_val.shape[0],img_rows,img_cols,1)\n",
        "# print(y_train.shape)\n",
        "\n",
        "# print('x_train shape:', x_train.shape)\n",
        "# print(x_train.shape[0], 'train samples')\n",
        "# print(x_val.shape[0], 'validation samples')\n",
        "\n",
        "# # convert class vectors to binary class matrices\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "Y = keras.utils.to_categorical(Y,8)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5lkAX-OXDyu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9315d842-0f5b-405b-87ea-1a81ce192db6"
      },
      "source": [
        "fold_no = 1\n",
        "num_classes = 8\n",
        "for train,test in kfold.split(X,Y):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(16, kernel_size=(3,3), activation='relu', input_shape=(img_rows,img_cols,1)))\n",
        "  model.add(Conv2D(16, (3, 3),activation='relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(32, (3,3), padding='same',activation='relu'))\n",
        "  model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dense(16, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  # compilation of model\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
        "  print('------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  model.fit(X[train],Y[train], batch_size = 64, epochs = 200,verbose = 1, validation_data= (X[test],Y[test]))\n",
        "  scores = model.evaluate(X[test],Y[test],verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "  fold_no = fold_no + 1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Train on 2989 samples, validate on 1495 samples\n",
            "Epoch 1/200\n",
            "2989/2989 [==============================] - 10s 3ms/step - loss: 1.9878 - accuracy: 0.3175 - val_loss: 1.6638 - val_accuracy: 0.4923\n",
            "Epoch 2/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.6707 - accuracy: 0.4015 - val_loss: 1.4599 - val_accuracy: 0.5130\n",
            "Epoch 3/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.5745 - accuracy: 0.4527 - val_loss: 1.2911 - val_accuracy: 0.5532\n",
            "Epoch 4/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.4960 - accuracy: 0.4968 - val_loss: 1.2366 - val_accuracy: 0.5645\n",
            "Epoch 5/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.4312 - accuracy: 0.5256 - val_loss: 1.2502 - val_accuracy: 0.5960\n",
            "Epoch 6/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.3884 - accuracy: 0.5453 - val_loss: 1.1532 - val_accuracy: 0.6060\n",
            "Epoch 7/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.3412 - accuracy: 0.5544 - val_loss: 1.1338 - val_accuracy: 0.6408\n",
            "Epoch 8/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.3021 - accuracy: 0.5580 - val_loss: 1.0757 - val_accuracy: 0.6582\n",
            "Epoch 9/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.2849 - accuracy: 0.5694 - val_loss: 1.1463 - val_accuracy: 0.6635\n",
            "Epoch 10/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.2435 - accuracy: 0.5731 - val_loss: 1.0149 - val_accuracy: 0.6589\n",
            "Epoch 11/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.2088 - accuracy: 0.5831 - val_loss: 1.0802 - val_accuracy: 0.6221\n",
            "Epoch 12/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.2036 - accuracy: 0.5825 - val_loss: 1.0056 - val_accuracy: 0.6622\n",
            "Epoch 13/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.1528 - accuracy: 0.6009 - val_loss: 0.9825 - val_accuracy: 0.6736\n",
            "Epoch 14/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.1226 - accuracy: 0.5975 - val_loss: 1.0171 - val_accuracy: 0.6522\n",
            "Epoch 15/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.1521 - accuracy: 0.5995 - val_loss: 0.9297 - val_accuracy: 0.6870\n",
            "Epoch 16/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.1059 - accuracy: 0.6102 - val_loss: 0.9393 - val_accuracy: 0.6836\n",
            "Epoch 17/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.1218 - accuracy: 0.6096 - val_loss: 0.9128 - val_accuracy: 0.6883\n",
            "Epoch 18/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0886 - accuracy: 0.6243 - val_loss: 0.9299 - val_accuracy: 0.6870\n",
            "Epoch 19/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0666 - accuracy: 0.6223 - val_loss: 0.9341 - val_accuracy: 0.6943\n",
            "Epoch 20/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0754 - accuracy: 0.6193 - val_loss: 0.9591 - val_accuracy: 0.6957\n",
            "Epoch 21/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0519 - accuracy: 0.6303 - val_loss: 0.9439 - val_accuracy: 0.6783\n",
            "Epoch 22/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0710 - accuracy: 0.6246 - val_loss: 0.8991 - val_accuracy: 0.6963\n",
            "Epoch 23/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0686 - accuracy: 0.6240 - val_loss: 0.8979 - val_accuracy: 0.6930\n",
            "Epoch 24/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0272 - accuracy: 0.6280 - val_loss: 0.9011 - val_accuracy: 0.6997\n",
            "Epoch 25/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0159 - accuracy: 0.6487 - val_loss: 0.9260 - val_accuracy: 0.6950\n",
            "Epoch 26/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0149 - accuracy: 0.6367 - val_loss: 0.8880 - val_accuracy: 0.7117\n",
            "Epoch 27/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0106 - accuracy: 0.6316 - val_loss: 0.9275 - val_accuracy: 0.6963\n",
            "Epoch 28/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0252 - accuracy: 0.6387 - val_loss: 0.8912 - val_accuracy: 0.7084\n",
            "Epoch 29/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0058 - accuracy: 0.6407 - val_loss: 0.9014 - val_accuracy: 0.7104\n",
            "Epoch 30/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0176 - accuracy: 0.6337 - val_loss: 0.9217 - val_accuracy: 0.6923\n",
            "Epoch 31/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0079 - accuracy: 0.6360 - val_loss: 0.8876 - val_accuracy: 0.7157\n",
            "Epoch 32/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9930 - accuracy: 0.6414 - val_loss: 0.9000 - val_accuracy: 0.7124\n",
            "Epoch 33/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9655 - accuracy: 0.6564 - val_loss: 0.8945 - val_accuracy: 0.7177\n",
            "Epoch 34/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0017 - accuracy: 0.6377 - val_loss: 0.8897 - val_accuracy: 0.6977\n",
            "Epoch 35/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9882 - accuracy: 0.6387 - val_loss: 0.9210 - val_accuracy: 0.7043\n",
            "Epoch 36/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9713 - accuracy: 0.6501 - val_loss: 0.8962 - val_accuracy: 0.7171\n",
            "Epoch 37/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9718 - accuracy: 0.6414 - val_loss: 0.9263 - val_accuracy: 0.7090\n",
            "Epoch 38/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9619 - accuracy: 0.6624 - val_loss: 0.8887 - val_accuracy: 0.7177\n",
            "Epoch 39/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9578 - accuracy: 0.6544 - val_loss: 0.8885 - val_accuracy: 0.7050\n",
            "Epoch 40/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9607 - accuracy: 0.6531 - val_loss: 0.8888 - val_accuracy: 0.7171\n",
            "Epoch 41/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9726 - accuracy: 0.6564 - val_loss: 0.8798 - val_accuracy: 0.7191\n",
            "Epoch 42/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9368 - accuracy: 0.6577 - val_loss: 0.8804 - val_accuracy: 0.7284\n",
            "Epoch 43/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9539 - accuracy: 0.6537 - val_loss: 0.8802 - val_accuracy: 0.7304\n",
            "Epoch 44/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9439 - accuracy: 0.6521 - val_loss: 0.8541 - val_accuracy: 0.7351\n",
            "Epoch 45/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9402 - accuracy: 0.6644 - val_loss: 0.8818 - val_accuracy: 0.7278\n",
            "Epoch 46/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9529 - accuracy: 0.6527 - val_loss: 0.8658 - val_accuracy: 0.7398\n",
            "Epoch 47/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9394 - accuracy: 0.6561 - val_loss: 0.8518 - val_accuracy: 0.7211\n",
            "Epoch 48/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9310 - accuracy: 0.6577 - val_loss: 0.8779 - val_accuracy: 0.7398\n",
            "Epoch 49/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9084 - accuracy: 0.6644 - val_loss: 0.8587 - val_accuracy: 0.7371\n",
            "Epoch 50/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8968 - accuracy: 0.6698 - val_loss: 0.8438 - val_accuracy: 0.7418\n",
            "Epoch 51/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9054 - accuracy: 0.6768 - val_loss: 0.8854 - val_accuracy: 0.7338\n",
            "Epoch 52/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9063 - accuracy: 0.6728 - val_loss: 0.8680 - val_accuracy: 0.7532\n",
            "Epoch 53/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8961 - accuracy: 0.6775 - val_loss: 0.9786 - val_accuracy: 0.7351\n",
            "Epoch 54/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8993 - accuracy: 0.6708 - val_loss: 0.8999 - val_accuracy: 0.7097\n",
            "Epoch 55/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9016 - accuracy: 0.6765 - val_loss: 0.9067 - val_accuracy: 0.7130\n",
            "Epoch 56/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8969 - accuracy: 0.6782 - val_loss: 0.8494 - val_accuracy: 0.7512\n",
            "Epoch 57/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8943 - accuracy: 0.6755 - val_loss: 0.8615 - val_accuracy: 0.7652\n",
            "Epoch 58/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8502 - accuracy: 0.6942 - val_loss: 0.8984 - val_accuracy: 0.7438\n",
            "Epoch 59/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8685 - accuracy: 0.6828 - val_loss: 0.8682 - val_accuracy: 0.7505\n",
            "Epoch 60/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8785 - accuracy: 0.6812 - val_loss: 0.8604 - val_accuracy: 0.7485\n",
            "Epoch 61/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8710 - accuracy: 0.6925 - val_loss: 0.8638 - val_accuracy: 0.7585\n",
            "Epoch 62/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8617 - accuracy: 0.6972 - val_loss: 0.8806 - val_accuracy: 0.7659\n",
            "Epoch 63/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8534 - accuracy: 0.6949 - val_loss: 0.8779 - val_accuracy: 0.7612\n",
            "Epoch 64/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8570 - accuracy: 0.6932 - val_loss: 0.8667 - val_accuracy: 0.7579\n",
            "Epoch 65/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8529 - accuracy: 0.6989 - val_loss: 0.8689 - val_accuracy: 0.7572\n",
            "Epoch 66/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8390 - accuracy: 0.7063 - val_loss: 0.8880 - val_accuracy: 0.7659\n",
            "Epoch 67/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8607 - accuracy: 0.6942 - val_loss: 0.8552 - val_accuracy: 0.7592\n",
            "Epoch 68/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8287 - accuracy: 0.6992 - val_loss: 0.8938 - val_accuracy: 0.7679\n",
            "Epoch 69/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8310 - accuracy: 0.7096 - val_loss: 0.8619 - val_accuracy: 0.7492\n",
            "Epoch 70/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8122 - accuracy: 0.7079 - val_loss: 0.9127 - val_accuracy: 0.7291\n",
            "Epoch 71/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8389 - accuracy: 0.6952 - val_loss: 0.8889 - val_accuracy: 0.7739\n",
            "Epoch 72/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8215 - accuracy: 0.7049 - val_loss: 0.9049 - val_accuracy: 0.7706\n",
            "Epoch 73/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8026 - accuracy: 0.7053 - val_loss: 0.9089 - val_accuracy: 0.7732\n",
            "Epoch 74/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7991 - accuracy: 0.7230 - val_loss: 0.9001 - val_accuracy: 0.7679\n",
            "Epoch 75/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8245 - accuracy: 0.7069 - val_loss: 0.8791 - val_accuracy: 0.7739\n",
            "Epoch 76/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7922 - accuracy: 0.7119 - val_loss: 0.8635 - val_accuracy: 0.7679\n",
            "Epoch 77/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7797 - accuracy: 0.7126 - val_loss: 0.8629 - val_accuracy: 0.7686\n",
            "Epoch 78/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7586 - accuracy: 0.7200 - val_loss: 0.8986 - val_accuracy: 0.7692\n",
            "Epoch 79/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7901 - accuracy: 0.7180 - val_loss: 0.8682 - val_accuracy: 0.7766\n",
            "Epoch 80/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7591 - accuracy: 0.7240 - val_loss: 0.8754 - val_accuracy: 0.7739\n",
            "Epoch 81/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7612 - accuracy: 0.7263 - val_loss: 0.8661 - val_accuracy: 0.7826\n",
            "Epoch 82/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7941 - accuracy: 0.7150 - val_loss: 0.9406 - val_accuracy: 0.7819\n",
            "Epoch 83/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7697 - accuracy: 0.7237 - val_loss: 0.8700 - val_accuracy: 0.7826\n",
            "Epoch 84/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7491 - accuracy: 0.7253 - val_loss: 0.9184 - val_accuracy: 0.7719\n",
            "Epoch 85/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7632 - accuracy: 0.7173 - val_loss: 0.9431 - val_accuracy: 0.7819\n",
            "Epoch 86/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7732 - accuracy: 0.7106 - val_loss: 0.9690 - val_accuracy: 0.7880\n",
            "Epoch 87/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7461 - accuracy: 0.7257 - val_loss: 0.9314 - val_accuracy: 0.7773\n",
            "Epoch 88/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7533 - accuracy: 0.7173 - val_loss: 0.9321 - val_accuracy: 0.7766\n",
            "Epoch 89/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7724 - accuracy: 0.7150 - val_loss: 0.9152 - val_accuracy: 0.7880\n",
            "Epoch 90/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7404 - accuracy: 0.7243 - val_loss: 1.0107 - val_accuracy: 0.7886\n",
            "Epoch 91/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7615 - accuracy: 0.7230 - val_loss: 0.9162 - val_accuracy: 0.7799\n",
            "Epoch 92/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7650 - accuracy: 0.7260 - val_loss: 0.9129 - val_accuracy: 0.7833\n",
            "Epoch 93/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7426 - accuracy: 0.7320 - val_loss: 0.9192 - val_accuracy: 0.7746\n",
            "Epoch 94/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7251 - accuracy: 0.7354 - val_loss: 0.9208 - val_accuracy: 0.7886\n",
            "Epoch 95/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7271 - accuracy: 0.7347 - val_loss: 1.0317 - val_accuracy: 0.7833\n",
            "Epoch 96/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7341 - accuracy: 0.7273 - val_loss: 0.9236 - val_accuracy: 0.7846\n",
            "Epoch 97/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7227 - accuracy: 0.7414 - val_loss: 0.8994 - val_accuracy: 0.7712\n",
            "Epoch 98/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7490 - accuracy: 0.7263 - val_loss: 0.8797 - val_accuracy: 0.7773\n",
            "Epoch 99/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7316 - accuracy: 0.7283 - val_loss: 0.9234 - val_accuracy: 0.7813\n",
            "Epoch 100/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7168 - accuracy: 0.7377 - val_loss: 0.9143 - val_accuracy: 0.7860\n",
            "Epoch 101/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7120 - accuracy: 0.7447 - val_loss: 1.0098 - val_accuracy: 0.7753\n",
            "Epoch 102/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7265 - accuracy: 0.7334 - val_loss: 0.9958 - val_accuracy: 0.7833\n",
            "Epoch 103/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7258 - accuracy: 0.7384 - val_loss: 1.0376 - val_accuracy: 0.7692\n",
            "Epoch 104/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7139 - accuracy: 0.7310 - val_loss: 0.9878 - val_accuracy: 0.7866\n",
            "Epoch 105/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7077 - accuracy: 0.7327 - val_loss: 0.9423 - val_accuracy: 0.7759\n",
            "Epoch 106/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7277 - accuracy: 0.7293 - val_loss: 0.9635 - val_accuracy: 0.7886\n",
            "Epoch 107/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7273 - accuracy: 0.7283 - val_loss: 0.9867 - val_accuracy: 0.7666\n",
            "Epoch 108/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7018 - accuracy: 0.7411 - val_loss: 0.9641 - val_accuracy: 0.7793\n",
            "Epoch 109/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7291 - accuracy: 0.7270 - val_loss: 0.9194 - val_accuracy: 0.7860\n",
            "Epoch 110/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6995 - accuracy: 0.7340 - val_loss: 0.9696 - val_accuracy: 0.7853\n",
            "Epoch 111/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7049 - accuracy: 0.7457 - val_loss: 0.9578 - val_accuracy: 0.7819\n",
            "Epoch 112/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7036 - accuracy: 0.7384 - val_loss: 0.9668 - val_accuracy: 0.7692\n",
            "Epoch 113/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6966 - accuracy: 0.7374 - val_loss: 0.9957 - val_accuracy: 0.7793\n",
            "Epoch 114/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6814 - accuracy: 0.7427 - val_loss: 1.0310 - val_accuracy: 0.7739\n",
            "Epoch 115/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6764 - accuracy: 0.7504 - val_loss: 1.0913 - val_accuracy: 0.7873\n",
            "Epoch 116/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6758 - accuracy: 0.7407 - val_loss: 1.0460 - val_accuracy: 0.7833\n",
            "Epoch 117/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7032 - accuracy: 0.7404 - val_loss: 0.9630 - val_accuracy: 0.7880\n",
            "Epoch 118/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6773 - accuracy: 0.7511 - val_loss: 0.9690 - val_accuracy: 0.7786\n",
            "Epoch 119/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6917 - accuracy: 0.7360 - val_loss: 1.0669 - val_accuracy: 0.7839\n",
            "Epoch 120/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6855 - accuracy: 0.7447 - val_loss: 0.9279 - val_accuracy: 0.7779\n",
            "Epoch 121/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6796 - accuracy: 0.7404 - val_loss: 1.0150 - val_accuracy: 0.7900\n",
            "Epoch 122/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6825 - accuracy: 0.7427 - val_loss: 0.9701 - val_accuracy: 0.7813\n",
            "Epoch 123/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6822 - accuracy: 0.7447 - val_loss: 0.9974 - val_accuracy: 0.7839\n",
            "Epoch 124/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6918 - accuracy: 0.7397 - val_loss: 0.9848 - val_accuracy: 0.7706\n",
            "Epoch 125/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6881 - accuracy: 0.7407 - val_loss: 0.9417 - val_accuracy: 0.7873\n",
            "Epoch 126/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7048 - accuracy: 0.7387 - val_loss: 0.9130 - val_accuracy: 0.7826\n",
            "Epoch 127/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6701 - accuracy: 0.7487 - val_loss: 0.9603 - val_accuracy: 0.7839\n",
            "Epoch 128/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6900 - accuracy: 0.7431 - val_loss: 0.9759 - val_accuracy: 0.7900\n",
            "Epoch 129/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6584 - accuracy: 0.7471 - val_loss: 0.9834 - val_accuracy: 0.7846\n",
            "Epoch 130/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6953 - accuracy: 0.7407 - val_loss: 0.9649 - val_accuracy: 0.7813\n",
            "Epoch 131/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6500 - accuracy: 0.7551 - val_loss: 1.0099 - val_accuracy: 0.7759\n",
            "Epoch 132/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6870 - accuracy: 0.7427 - val_loss: 0.9997 - val_accuracy: 0.7732\n",
            "Epoch 133/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6758 - accuracy: 0.7447 - val_loss: 0.9872 - val_accuracy: 0.7813\n",
            "Epoch 134/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6555 - accuracy: 0.7598 - val_loss: 1.0348 - val_accuracy: 0.7866\n",
            "Epoch 135/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6530 - accuracy: 0.7541 - val_loss: 1.0551 - val_accuracy: 0.7759\n",
            "Epoch 136/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6752 - accuracy: 0.7551 - val_loss: 1.0471 - val_accuracy: 0.7766\n",
            "Epoch 137/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6949 - accuracy: 0.7347 - val_loss: 0.9856 - val_accuracy: 0.7766\n",
            "Epoch 138/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6405 - accuracy: 0.7631 - val_loss: 0.9968 - val_accuracy: 0.7779\n",
            "Epoch 139/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6823 - accuracy: 0.7504 - val_loss: 0.9577 - val_accuracy: 0.7839\n",
            "Epoch 140/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6420 - accuracy: 0.7608 - val_loss: 0.9824 - val_accuracy: 0.7886\n",
            "Epoch 141/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6449 - accuracy: 0.7551 - val_loss: 1.0000 - val_accuracy: 0.7833\n",
            "Epoch 142/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6530 - accuracy: 0.7548 - val_loss: 1.0496 - val_accuracy: 0.7933\n",
            "Epoch 143/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6664 - accuracy: 0.7588 - val_loss: 1.0582 - val_accuracy: 0.7940\n",
            "Epoch 144/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6527 - accuracy: 0.7608 - val_loss: 1.0572 - val_accuracy: 0.7853\n",
            "Epoch 145/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6659 - accuracy: 0.7571 - val_loss: 0.9509 - val_accuracy: 0.7806\n",
            "Epoch 146/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6471 - accuracy: 0.7544 - val_loss: 1.0011 - val_accuracy: 0.7900\n",
            "Epoch 147/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6549 - accuracy: 0.7464 - val_loss: 1.0679 - val_accuracy: 0.7940\n",
            "Epoch 148/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6386 - accuracy: 0.7715 - val_loss: 1.0382 - val_accuracy: 0.7753\n",
            "Epoch 149/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6346 - accuracy: 0.7658 - val_loss: 0.9784 - val_accuracy: 0.7739\n",
            "Epoch 150/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6464 - accuracy: 0.7635 - val_loss: 1.0236 - val_accuracy: 0.7893\n",
            "Epoch 151/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6604 - accuracy: 0.7591 - val_loss: 1.0474 - val_accuracy: 0.7773\n",
            "Epoch 152/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6355 - accuracy: 0.7698 - val_loss: 0.9687 - val_accuracy: 0.7806\n",
            "Epoch 153/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6422 - accuracy: 0.7504 - val_loss: 1.0222 - val_accuracy: 0.7866\n",
            "Epoch 154/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6447 - accuracy: 0.7601 - val_loss: 0.9782 - val_accuracy: 0.7839\n",
            "Epoch 155/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6222 - accuracy: 0.7712 - val_loss: 1.0341 - val_accuracy: 0.7866\n",
            "Epoch 156/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6303 - accuracy: 0.7681 - val_loss: 0.9717 - val_accuracy: 0.7913\n",
            "Epoch 157/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6406 - accuracy: 0.7695 - val_loss: 1.0577 - val_accuracy: 0.7773\n",
            "Epoch 158/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6287 - accuracy: 0.7598 - val_loss: 1.0768 - val_accuracy: 0.7759\n",
            "Epoch 159/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6435 - accuracy: 0.7661 - val_loss: 1.0868 - val_accuracy: 0.7793\n",
            "Epoch 160/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6242 - accuracy: 0.7718 - val_loss: 1.1055 - val_accuracy: 0.7846\n",
            "Epoch 161/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6285 - accuracy: 0.7668 - val_loss: 1.1930 - val_accuracy: 0.7806\n",
            "Epoch 162/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6174 - accuracy: 0.7732 - val_loss: 1.0909 - val_accuracy: 0.7833\n",
            "Epoch 163/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6308 - accuracy: 0.7685 - val_loss: 1.1546 - val_accuracy: 0.7753\n",
            "Epoch 164/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6207 - accuracy: 0.7705 - val_loss: 1.1036 - val_accuracy: 0.7926\n",
            "Epoch 165/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6233 - accuracy: 0.7732 - val_loss: 1.1696 - val_accuracy: 0.7953\n",
            "Epoch 166/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6125 - accuracy: 0.7651 - val_loss: 1.0496 - val_accuracy: 0.7933\n",
            "Epoch 167/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6115 - accuracy: 0.7688 - val_loss: 1.0751 - val_accuracy: 0.7853\n",
            "Epoch 168/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6165 - accuracy: 0.7671 - val_loss: 1.1794 - val_accuracy: 0.7953\n",
            "Epoch 169/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6002 - accuracy: 0.7845 - val_loss: 1.0874 - val_accuracy: 0.7960\n",
            "Epoch 170/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6251 - accuracy: 0.7748 - val_loss: 1.0648 - val_accuracy: 0.7900\n",
            "Epoch 171/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6122 - accuracy: 0.7799 - val_loss: 1.0492 - val_accuracy: 0.7866\n",
            "Epoch 172/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6051 - accuracy: 0.7745 - val_loss: 1.2320 - val_accuracy: 0.7906\n",
            "Epoch 173/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6106 - accuracy: 0.7732 - val_loss: 1.2180 - val_accuracy: 0.7913\n",
            "Epoch 174/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6119 - accuracy: 0.7779 - val_loss: 1.1734 - val_accuracy: 0.7913\n",
            "Epoch 175/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5801 - accuracy: 0.7896 - val_loss: 1.1409 - val_accuracy: 0.7913\n",
            "Epoch 176/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6091 - accuracy: 0.7886 - val_loss: 1.2023 - val_accuracy: 0.7826\n",
            "Epoch 177/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6017 - accuracy: 0.7789 - val_loss: 1.1699 - val_accuracy: 0.7953\n",
            "Epoch 178/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6256 - accuracy: 0.7715 - val_loss: 1.0925 - val_accuracy: 0.7873\n",
            "Epoch 179/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5858 - accuracy: 0.7896 - val_loss: 1.2202 - val_accuracy: 0.7886\n",
            "Epoch 180/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5986 - accuracy: 0.7772 - val_loss: 1.0876 - val_accuracy: 0.7913\n",
            "Epoch 181/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5933 - accuracy: 0.7839 - val_loss: 1.0880 - val_accuracy: 0.7960\n",
            "Epoch 182/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5774 - accuracy: 0.7845 - val_loss: 1.1449 - val_accuracy: 0.7940\n",
            "Epoch 183/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6053 - accuracy: 0.7822 - val_loss: 1.1116 - val_accuracy: 0.7853\n",
            "Epoch 184/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5805 - accuracy: 0.7892 - val_loss: 1.1651 - val_accuracy: 0.7953\n",
            "Epoch 185/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5858 - accuracy: 0.7849 - val_loss: 1.1183 - val_accuracy: 0.7779\n",
            "Epoch 186/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5957 - accuracy: 0.7852 - val_loss: 1.0805 - val_accuracy: 0.7967\n",
            "Epoch 187/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5913 - accuracy: 0.7973 - val_loss: 1.0427 - val_accuracy: 0.7906\n",
            "Epoch 188/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5941 - accuracy: 0.7832 - val_loss: 1.1956 - val_accuracy: 0.7900\n",
            "Epoch 189/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5898 - accuracy: 0.7879 - val_loss: 1.1683 - val_accuracy: 0.7960\n",
            "Epoch 190/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5573 - accuracy: 0.8006 - val_loss: 1.1636 - val_accuracy: 0.7960\n",
            "Epoch 191/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5731 - accuracy: 0.7963 - val_loss: 1.1165 - val_accuracy: 0.7967\n",
            "Epoch 192/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5680 - accuracy: 0.7979 - val_loss: 1.2160 - val_accuracy: 0.8007\n",
            "Epoch 193/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5765 - accuracy: 0.7929 - val_loss: 1.5307 - val_accuracy: 0.7946\n",
            "Epoch 194/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5746 - accuracy: 0.7909 - val_loss: 1.1343 - val_accuracy: 0.7993\n",
            "Epoch 195/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5812 - accuracy: 0.7959 - val_loss: 1.1063 - val_accuracy: 0.8033\n",
            "Epoch 196/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5588 - accuracy: 0.8009 - val_loss: 1.1427 - val_accuracy: 0.7933\n",
            "Epoch 197/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5924 - accuracy: 0.7839 - val_loss: 1.2312 - val_accuracy: 0.7933\n",
            "Epoch 198/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5737 - accuracy: 0.7889 - val_loss: 1.0736 - val_accuracy: 0.7860\n",
            "Epoch 199/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5672 - accuracy: 0.7922 - val_loss: 1.2416 - val_accuracy: 0.7933\n",
            "Epoch 200/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5689 - accuracy: 0.7926 - val_loss: 1.1562 - val_accuracy: 0.7933\n",
            "Score for fold 1: loss of 1.1561970226342064; accuracy of 79.3311059474945%\n",
            "------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Train on 2989 samples, validate on 1495 samples\n",
            "Epoch 1/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.9987 - accuracy: 0.4008 - val_loss: 1.6929 - val_accuracy: 0.4120\n",
            "Epoch 2/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.7094 - accuracy: 0.4473 - val_loss: 1.5500 - val_accuracy: 0.4910\n",
            "Epoch 3/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.6040 - accuracy: 0.4885 - val_loss: 1.4708 - val_accuracy: 0.5050\n",
            "Epoch 4/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.5596 - accuracy: 0.5022 - val_loss: 1.4642 - val_accuracy: 0.4863\n",
            "Epoch 5/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.5173 - accuracy: 0.5095 - val_loss: 1.4435 - val_accuracy: 0.4816\n",
            "Epoch 6/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.5027 - accuracy: 0.5149 - val_loss: 1.3999 - val_accuracy: 0.5050\n",
            "Epoch 7/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.4671 - accuracy: 0.5253 - val_loss: 1.3380 - val_accuracy: 0.5224\n",
            "Epoch 8/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.4241 - accuracy: 0.5376 - val_loss: 1.3175 - val_accuracy: 0.5779\n",
            "Epoch 9/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.3923 - accuracy: 0.5473 - val_loss: 1.3296 - val_accuracy: 0.5565\n",
            "Epoch 10/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.3656 - accuracy: 0.5564 - val_loss: 1.2775 - val_accuracy: 0.5880\n",
            "Epoch 11/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.3466 - accuracy: 0.5647 - val_loss: 1.2266 - val_accuracy: 0.5940\n",
            "Epoch 12/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.3014 - accuracy: 0.5754 - val_loss: 1.2181 - val_accuracy: 0.6040\n",
            "Epoch 13/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.2528 - accuracy: 0.5808 - val_loss: 1.1612 - val_accuracy: 0.6007\n",
            "Epoch 14/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.2637 - accuracy: 0.5731 - val_loss: 1.1641 - val_accuracy: 0.5880\n",
            "Epoch 15/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.2061 - accuracy: 0.5935 - val_loss: 1.1331 - val_accuracy: 0.6027\n",
            "Epoch 16/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.1846 - accuracy: 0.5905 - val_loss: 1.1239 - val_accuracy: 0.6020\n",
            "Epoch 17/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.1582 - accuracy: 0.6022 - val_loss: 1.1662 - val_accuracy: 0.5692\n",
            "Epoch 18/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.1554 - accuracy: 0.5979 - val_loss: 1.0760 - val_accuracy: 0.6181\n",
            "Epoch 19/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.1457 - accuracy: 0.5999 - val_loss: 1.0525 - val_accuracy: 0.6281\n",
            "Epoch 20/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.1249 - accuracy: 0.6052 - val_loss: 1.1124 - val_accuracy: 0.6094\n",
            "Epoch 21/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.1100 - accuracy: 0.6143 - val_loss: 1.0581 - val_accuracy: 0.6227\n",
            "Epoch 22/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0819 - accuracy: 0.6189 - val_loss: 1.0726 - val_accuracy: 0.6495\n",
            "Epoch 23/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0769 - accuracy: 0.6236 - val_loss: 1.0424 - val_accuracy: 0.6642\n",
            "Epoch 24/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0559 - accuracy: 0.6306 - val_loss: 1.0498 - val_accuracy: 0.6448\n",
            "Epoch 25/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0416 - accuracy: 0.6303 - val_loss: 1.0383 - val_accuracy: 0.6508\n",
            "Epoch 26/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0321 - accuracy: 0.6490 - val_loss: 1.0404 - val_accuracy: 0.6515\n",
            "Epoch 27/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0546 - accuracy: 0.6337 - val_loss: 1.0265 - val_accuracy: 0.6555\n",
            "Epoch 28/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0344 - accuracy: 0.6403 - val_loss: 1.0311 - val_accuracy: 0.6569\n",
            "Epoch 29/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0019 - accuracy: 0.6490 - val_loss: 1.0657 - val_accuracy: 0.6562\n",
            "Epoch 30/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 1.0054 - accuracy: 0.6497 - val_loss: 1.0338 - val_accuracy: 0.6736\n",
            "Epoch 31/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9914 - accuracy: 0.6511 - val_loss: 1.0360 - val_accuracy: 0.6696\n",
            "Epoch 32/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9947 - accuracy: 0.6524 - val_loss: 1.0136 - val_accuracy: 0.6749\n",
            "Epoch 33/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9943 - accuracy: 0.6648 - val_loss: 1.0026 - val_accuracy: 0.6722\n",
            "Epoch 34/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9964 - accuracy: 0.6517 - val_loss: 1.0178 - val_accuracy: 0.6789\n",
            "Epoch 35/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9682 - accuracy: 0.6658 - val_loss: 1.0292 - val_accuracy: 0.6809\n",
            "Epoch 36/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9810 - accuracy: 0.6664 - val_loss: 1.0331 - val_accuracy: 0.6462\n",
            "Epoch 37/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9537 - accuracy: 0.6685 - val_loss: 1.0844 - val_accuracy: 0.6776\n",
            "Epoch 38/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9469 - accuracy: 0.6698 - val_loss: 1.0065 - val_accuracy: 0.6642\n",
            "Epoch 39/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9520 - accuracy: 0.6738 - val_loss: 1.0089 - val_accuracy: 0.6763\n",
            "Epoch 40/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9468 - accuracy: 0.6731 - val_loss: 1.0305 - val_accuracy: 0.6589\n",
            "Epoch 41/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9375 - accuracy: 0.6731 - val_loss: 1.0226 - val_accuracy: 0.6555\n",
            "Epoch 42/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9339 - accuracy: 0.6728 - val_loss: 1.0482 - val_accuracy: 0.6682\n",
            "Epoch 43/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9193 - accuracy: 0.6858 - val_loss: 0.9827 - val_accuracy: 0.6796\n",
            "Epoch 44/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9204 - accuracy: 0.6728 - val_loss: 0.9985 - val_accuracy: 0.6789\n",
            "Epoch 45/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9310 - accuracy: 0.6822 - val_loss: 0.9911 - val_accuracy: 0.6796\n",
            "Epoch 46/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9112 - accuracy: 0.6879 - val_loss: 0.9914 - val_accuracy: 0.6696\n",
            "Epoch 47/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9144 - accuracy: 0.6959 - val_loss: 0.9726 - val_accuracy: 0.6916\n",
            "Epoch 48/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9025 - accuracy: 0.6969 - val_loss: 0.9817 - val_accuracy: 0.7030\n",
            "Epoch 49/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8985 - accuracy: 0.6952 - val_loss: 0.9660 - val_accuracy: 0.6930\n",
            "Epoch 50/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8825 - accuracy: 0.6915 - val_loss: 0.9687 - val_accuracy: 0.6823\n",
            "Epoch 51/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.9020 - accuracy: 0.6962 - val_loss: 1.0212 - val_accuracy: 0.6656\n",
            "Epoch 52/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8752 - accuracy: 0.7053 - val_loss: 0.9640 - val_accuracy: 0.7017\n",
            "Epoch 53/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8813 - accuracy: 0.6986 - val_loss: 0.9886 - val_accuracy: 0.7037\n",
            "Epoch 54/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8828 - accuracy: 0.6962 - val_loss: 0.9888 - val_accuracy: 0.7030\n",
            "Epoch 55/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8621 - accuracy: 0.7056 - val_loss: 1.0151 - val_accuracy: 0.6957\n",
            "Epoch 56/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8615 - accuracy: 0.7026 - val_loss: 0.9639 - val_accuracy: 0.6896\n",
            "Epoch 57/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8504 - accuracy: 0.7166 - val_loss: 0.9618 - val_accuracy: 0.7117\n",
            "Epoch 58/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8329 - accuracy: 0.7113 - val_loss: 0.9804 - val_accuracy: 0.7311\n",
            "Epoch 59/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8479 - accuracy: 0.7116 - val_loss: 1.0074 - val_accuracy: 0.7124\n",
            "Epoch 60/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8682 - accuracy: 0.7046 - val_loss: 0.9615 - val_accuracy: 0.7110\n",
            "Epoch 61/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8431 - accuracy: 0.7163 - val_loss: 0.9605 - val_accuracy: 0.7151\n",
            "Epoch 62/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8480 - accuracy: 0.7119 - val_loss: 0.9832 - val_accuracy: 0.7197\n",
            "Epoch 63/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8255 - accuracy: 0.7196 - val_loss: 0.9737 - val_accuracy: 0.7064\n",
            "Epoch 64/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8274 - accuracy: 0.7247 - val_loss: 0.9805 - val_accuracy: 0.6936\n",
            "Epoch 65/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8255 - accuracy: 0.7203 - val_loss: 0.9764 - val_accuracy: 0.7003\n",
            "Epoch 66/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8283 - accuracy: 0.7260 - val_loss: 0.9710 - val_accuracy: 0.6997\n",
            "Epoch 67/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8076 - accuracy: 0.7327 - val_loss: 1.0028 - val_accuracy: 0.7197\n",
            "Epoch 68/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8226 - accuracy: 0.7213 - val_loss: 1.0104 - val_accuracy: 0.7057\n",
            "Epoch 69/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8075 - accuracy: 0.7280 - val_loss: 1.0146 - val_accuracy: 0.7237\n",
            "Epoch 70/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7838 - accuracy: 0.7297 - val_loss: 0.9628 - val_accuracy: 0.7090\n",
            "Epoch 71/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7902 - accuracy: 0.7337 - val_loss: 1.0192 - val_accuracy: 0.7211\n",
            "Epoch 72/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8210 - accuracy: 0.7233 - val_loss: 0.9258 - val_accuracy: 0.7304\n",
            "Epoch 73/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7891 - accuracy: 0.7303 - val_loss: 0.9627 - val_accuracy: 0.7304\n",
            "Epoch 74/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7804 - accuracy: 0.7377 - val_loss: 0.9708 - val_accuracy: 0.7171\n",
            "Epoch 75/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8074 - accuracy: 0.7364 - val_loss: 1.0182 - val_accuracy: 0.7244\n",
            "Epoch 76/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7911 - accuracy: 0.7340 - val_loss: 0.9941 - val_accuracy: 0.7284\n",
            "Epoch 77/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8088 - accuracy: 0.7327 - val_loss: 0.9790 - val_accuracy: 0.7264\n",
            "Epoch 78/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8052 - accuracy: 0.7337 - val_loss: 0.9558 - val_accuracy: 0.7378\n",
            "Epoch 79/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7888 - accuracy: 0.7310 - val_loss: 0.9559 - val_accuracy: 0.7237\n",
            "Epoch 80/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7969 - accuracy: 0.7324 - val_loss: 1.0145 - val_accuracy: 0.7264\n",
            "Epoch 81/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.8034 - accuracy: 0.7324 - val_loss: 1.0192 - val_accuracy: 0.7164\n",
            "Epoch 82/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7899 - accuracy: 0.7267 - val_loss: 0.9458 - val_accuracy: 0.7304\n",
            "Epoch 83/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7736 - accuracy: 0.7367 - val_loss: 0.9659 - val_accuracy: 0.7324\n",
            "Epoch 84/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7793 - accuracy: 0.7374 - val_loss: 1.0251 - val_accuracy: 0.7070\n",
            "Epoch 85/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7955 - accuracy: 0.7364 - val_loss: 1.0132 - val_accuracy: 0.7204\n",
            "Epoch 86/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7811 - accuracy: 0.7377 - val_loss: 0.9633 - val_accuracy: 0.7318\n",
            "Epoch 87/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7438 - accuracy: 0.7528 - val_loss: 1.0012 - val_accuracy: 0.7291\n",
            "Epoch 88/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7838 - accuracy: 0.7471 - val_loss: 0.9592 - val_accuracy: 0.7271\n",
            "Epoch 89/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7715 - accuracy: 0.7407 - val_loss: 0.9448 - val_accuracy: 0.7338\n",
            "Epoch 90/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7661 - accuracy: 0.7427 - val_loss: 0.9660 - val_accuracy: 0.7472\n",
            "Epoch 91/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7523 - accuracy: 0.7528 - val_loss: 0.9961 - val_accuracy: 0.7304\n",
            "Epoch 92/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7734 - accuracy: 0.7397 - val_loss: 1.0034 - val_accuracy: 0.7344\n",
            "Epoch 93/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7607 - accuracy: 0.7538 - val_loss: 1.0211 - val_accuracy: 0.7458\n",
            "Epoch 94/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7308 - accuracy: 0.7551 - val_loss: 0.9722 - val_accuracy: 0.7405\n",
            "Epoch 95/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7754 - accuracy: 0.7471 - val_loss: 0.9956 - val_accuracy: 0.7351\n",
            "Epoch 96/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7543 - accuracy: 0.7544 - val_loss: 0.9840 - val_accuracy: 0.7338\n",
            "Epoch 97/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7519 - accuracy: 0.7524 - val_loss: 0.9872 - val_accuracy: 0.7438\n",
            "Epoch 98/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7603 - accuracy: 0.7394 - val_loss: 0.9858 - val_accuracy: 0.7344\n",
            "Epoch 99/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7544 - accuracy: 0.7444 - val_loss: 1.0189 - val_accuracy: 0.7492\n",
            "Epoch 100/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7378 - accuracy: 0.7521 - val_loss: 0.9681 - val_accuracy: 0.7411\n",
            "Epoch 101/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7537 - accuracy: 0.7491 - val_loss: 0.9850 - val_accuracy: 0.7331\n",
            "Epoch 102/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7506 - accuracy: 0.7551 - val_loss: 0.9914 - val_accuracy: 0.7371\n",
            "Epoch 103/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7644 - accuracy: 0.7447 - val_loss: 0.9998 - val_accuracy: 0.7304\n",
            "Epoch 104/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7324 - accuracy: 0.7578 - val_loss: 0.9594 - val_accuracy: 0.7378\n",
            "Epoch 105/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7434 - accuracy: 0.7554 - val_loss: 1.0285 - val_accuracy: 0.7365\n",
            "Epoch 106/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7375 - accuracy: 0.7558 - val_loss: 0.9664 - val_accuracy: 0.7425\n",
            "Epoch 107/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7151 - accuracy: 0.7521 - val_loss: 0.9987 - val_accuracy: 0.7478\n",
            "Epoch 108/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7266 - accuracy: 0.7524 - val_loss: 0.9762 - val_accuracy: 0.7378\n",
            "Epoch 109/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7147 - accuracy: 0.7635 - val_loss: 0.9664 - val_accuracy: 0.7425\n",
            "Epoch 110/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7615 - accuracy: 0.7437 - val_loss: 0.9518 - val_accuracy: 0.7405\n",
            "Epoch 111/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7297 - accuracy: 0.7514 - val_loss: 0.9534 - val_accuracy: 0.7545\n",
            "Epoch 112/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7163 - accuracy: 0.7544 - val_loss: 0.9822 - val_accuracy: 0.7478\n",
            "Epoch 113/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7365 - accuracy: 0.7457 - val_loss: 0.9487 - val_accuracy: 0.7498\n",
            "Epoch 114/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7021 - accuracy: 0.7688 - val_loss: 1.0296 - val_accuracy: 0.7532\n",
            "Epoch 115/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7123 - accuracy: 0.7675 - val_loss: 0.9455 - val_accuracy: 0.7518\n",
            "Epoch 116/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7189 - accuracy: 0.7531 - val_loss: 0.9320 - val_accuracy: 0.7492\n",
            "Epoch 117/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7026 - accuracy: 0.7671 - val_loss: 0.9846 - val_accuracy: 0.7525\n",
            "Epoch 118/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7168 - accuracy: 0.7671 - val_loss: 0.9769 - val_accuracy: 0.7431\n",
            "Epoch 119/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7148 - accuracy: 0.7621 - val_loss: 0.9994 - val_accuracy: 0.7512\n",
            "Epoch 120/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7035 - accuracy: 0.7665 - val_loss: 1.0607 - val_accuracy: 0.7585\n",
            "Epoch 121/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7033 - accuracy: 0.7702 - val_loss: 0.9753 - val_accuracy: 0.7492\n",
            "Epoch 122/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7069 - accuracy: 0.7595 - val_loss: 0.9875 - val_accuracy: 0.7559\n",
            "Epoch 123/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7050 - accuracy: 0.7651 - val_loss: 0.9634 - val_accuracy: 0.7465\n",
            "Epoch 124/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7101 - accuracy: 0.7611 - val_loss: 0.9746 - val_accuracy: 0.7431\n",
            "Epoch 125/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7146 - accuracy: 0.7635 - val_loss: 1.0229 - val_accuracy: 0.7478\n",
            "Epoch 126/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7145 - accuracy: 0.7641 - val_loss: 0.9592 - val_accuracy: 0.7445\n",
            "Epoch 127/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7044 - accuracy: 0.7712 - val_loss: 1.0136 - val_accuracy: 0.7659\n",
            "Epoch 128/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7209 - accuracy: 0.7588 - val_loss: 0.9718 - val_accuracy: 0.7318\n",
            "Epoch 129/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6846 - accuracy: 0.7738 - val_loss: 0.9679 - val_accuracy: 0.7518\n",
            "Epoch 130/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.7091 - accuracy: 0.7665 - val_loss: 0.9566 - val_accuracy: 0.7505\n",
            "Epoch 131/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6812 - accuracy: 0.7708 - val_loss: 0.9909 - val_accuracy: 0.7478\n",
            "Epoch 132/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6798 - accuracy: 0.7728 - val_loss: 0.9730 - val_accuracy: 0.7485\n",
            "Epoch 133/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6903 - accuracy: 0.7678 - val_loss: 1.0208 - val_accuracy: 0.7552\n",
            "Epoch 134/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6708 - accuracy: 0.7671 - val_loss: 0.9409 - val_accuracy: 0.7478\n",
            "Epoch 135/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6807 - accuracy: 0.7708 - val_loss: 0.9881 - val_accuracy: 0.7418\n",
            "Epoch 136/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6705 - accuracy: 0.7809 - val_loss: 1.0321 - val_accuracy: 0.7605\n",
            "Epoch 137/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6812 - accuracy: 0.7635 - val_loss: 1.0309 - val_accuracy: 0.7538\n",
            "Epoch 138/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6833 - accuracy: 0.7725 - val_loss: 1.0226 - val_accuracy: 0.7351\n",
            "Epoch 139/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6676 - accuracy: 0.7772 - val_loss: 1.0582 - val_accuracy: 0.7472\n",
            "Epoch 140/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6789 - accuracy: 0.7692 - val_loss: 0.9514 - val_accuracy: 0.7518\n",
            "Epoch 141/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6891 - accuracy: 0.7655 - val_loss: 0.9743 - val_accuracy: 0.7472\n",
            "Epoch 142/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6815 - accuracy: 0.7698 - val_loss: 0.9500 - val_accuracy: 0.7472\n",
            "Epoch 143/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6797 - accuracy: 0.7685 - val_loss: 0.9547 - val_accuracy: 0.7518\n",
            "Epoch 144/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6575 - accuracy: 0.7738 - val_loss: 1.0308 - val_accuracy: 0.7525\n",
            "Epoch 145/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6898 - accuracy: 0.7661 - val_loss: 0.9613 - val_accuracy: 0.7612\n",
            "Epoch 146/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6722 - accuracy: 0.7835 - val_loss: 1.0198 - val_accuracy: 0.7398\n",
            "Epoch 147/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6701 - accuracy: 0.7822 - val_loss: 0.9653 - val_accuracy: 0.7438\n",
            "Epoch 148/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6477 - accuracy: 0.7886 - val_loss: 1.0346 - val_accuracy: 0.7485\n",
            "Epoch 149/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6676 - accuracy: 0.7765 - val_loss: 0.9857 - val_accuracy: 0.7532\n",
            "Epoch 150/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6487 - accuracy: 0.7745 - val_loss: 0.9604 - val_accuracy: 0.7472\n",
            "Epoch 151/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6586 - accuracy: 0.7792 - val_loss: 1.0833 - val_accuracy: 0.7632\n",
            "Epoch 152/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6621 - accuracy: 0.7715 - val_loss: 1.0310 - val_accuracy: 0.7599\n",
            "Epoch 153/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6492 - accuracy: 0.7829 - val_loss: 1.0268 - val_accuracy: 0.7492\n",
            "Epoch 154/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6607 - accuracy: 0.7755 - val_loss: 1.0272 - val_accuracy: 0.7585\n",
            "Epoch 155/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6583 - accuracy: 0.7755 - val_loss: 0.9683 - val_accuracy: 0.7579\n",
            "Epoch 156/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6464 - accuracy: 0.7765 - val_loss: 0.9863 - val_accuracy: 0.7625\n",
            "Epoch 157/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6586 - accuracy: 0.7742 - val_loss: 0.9883 - val_accuracy: 0.7518\n",
            "Epoch 158/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6389 - accuracy: 0.7855 - val_loss: 0.9946 - val_accuracy: 0.7478\n",
            "Epoch 159/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6136 - accuracy: 0.7889 - val_loss: 1.0125 - val_accuracy: 0.7652\n",
            "Epoch 160/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6323 - accuracy: 0.7805 - val_loss: 0.9771 - val_accuracy: 0.7559\n",
            "Epoch 161/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6403 - accuracy: 0.7782 - val_loss: 0.9756 - val_accuracy: 0.7485\n",
            "Epoch 162/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6393 - accuracy: 0.7795 - val_loss: 0.9729 - val_accuracy: 0.7512\n",
            "Epoch 163/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6287 - accuracy: 0.7815 - val_loss: 0.9795 - val_accuracy: 0.7512\n",
            "Epoch 164/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6355 - accuracy: 0.7772 - val_loss: 1.0347 - val_accuracy: 0.7619\n",
            "Epoch 165/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6362 - accuracy: 0.7825 - val_loss: 0.9893 - val_accuracy: 0.7545\n",
            "Epoch 166/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6167 - accuracy: 0.7922 - val_loss: 1.0209 - val_accuracy: 0.7645\n",
            "Epoch 167/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6252 - accuracy: 0.7845 - val_loss: 1.0211 - val_accuracy: 0.7605\n",
            "Epoch 168/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6283 - accuracy: 0.7799 - val_loss: 1.0537 - val_accuracy: 0.7612\n",
            "Epoch 169/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6019 - accuracy: 0.7899 - val_loss: 1.0040 - val_accuracy: 0.7645\n",
            "Epoch 170/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6172 - accuracy: 0.7912 - val_loss: 1.0414 - val_accuracy: 0.7592\n",
            "Epoch 171/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6174 - accuracy: 0.7906 - val_loss: 0.9875 - val_accuracy: 0.7565\n",
            "Epoch 172/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6325 - accuracy: 0.7855 - val_loss: 0.9869 - val_accuracy: 0.7538\n",
            "Epoch 173/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5969 - accuracy: 0.7936 - val_loss: 1.0494 - val_accuracy: 0.7559\n",
            "Epoch 174/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6297 - accuracy: 0.7839 - val_loss: 1.0326 - val_accuracy: 0.7565\n",
            "Epoch 175/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5951 - accuracy: 0.7902 - val_loss: 1.1067 - val_accuracy: 0.7552\n",
            "Epoch 176/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6066 - accuracy: 0.7829 - val_loss: 0.9746 - val_accuracy: 0.7532\n",
            "Epoch 177/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6025 - accuracy: 0.7862 - val_loss: 1.0115 - val_accuracy: 0.7512\n",
            "Epoch 178/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6257 - accuracy: 0.7738 - val_loss: 1.0200 - val_accuracy: 0.7498\n",
            "Epoch 179/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5912 - accuracy: 0.7989 - val_loss: 1.0415 - val_accuracy: 0.7699\n",
            "Epoch 180/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6117 - accuracy: 0.7876 - val_loss: 1.0278 - val_accuracy: 0.7579\n",
            "Epoch 181/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6112 - accuracy: 0.7889 - val_loss: 1.0447 - val_accuracy: 0.7739\n",
            "Epoch 182/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6021 - accuracy: 0.7996 - val_loss: 0.9766 - val_accuracy: 0.7545\n",
            "Epoch 183/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6013 - accuracy: 0.7855 - val_loss: 0.9992 - val_accuracy: 0.7565\n",
            "Epoch 184/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6083 - accuracy: 0.7899 - val_loss: 1.0467 - val_accuracy: 0.7612\n",
            "Epoch 185/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6062 - accuracy: 0.7909 - val_loss: 1.0640 - val_accuracy: 0.7699\n",
            "Epoch 186/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5893 - accuracy: 0.7979 - val_loss: 1.0407 - val_accuracy: 0.7679\n",
            "Epoch 187/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5898 - accuracy: 0.7966 - val_loss: 0.9989 - val_accuracy: 0.7612\n",
            "Epoch 188/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6004 - accuracy: 0.7969 - val_loss: 1.0294 - val_accuracy: 0.7579\n",
            "Epoch 189/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5905 - accuracy: 0.7979 - val_loss: 1.0540 - val_accuracy: 0.7612\n",
            "Epoch 190/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5892 - accuracy: 0.7959 - val_loss: 1.0731 - val_accuracy: 0.7739\n",
            "Epoch 191/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5833 - accuracy: 0.7952 - val_loss: 1.0165 - val_accuracy: 0.7645\n",
            "Epoch 192/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5815 - accuracy: 0.8006 - val_loss: 0.9947 - val_accuracy: 0.7552\n",
            "Epoch 193/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.6060 - accuracy: 0.7979 - val_loss: 0.9802 - val_accuracy: 0.7659\n",
            "Epoch 194/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5928 - accuracy: 0.7932 - val_loss: 1.0227 - val_accuracy: 0.7659\n",
            "Epoch 195/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5548 - accuracy: 0.7963 - val_loss: 1.0751 - val_accuracy: 0.7645\n",
            "Epoch 196/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5803 - accuracy: 0.8016 - val_loss: 1.0581 - val_accuracy: 0.7692\n",
            "Epoch 197/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5856 - accuracy: 0.8006 - val_loss: 0.9787 - val_accuracy: 0.7498\n",
            "Epoch 198/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5784 - accuracy: 0.8006 - val_loss: 1.0468 - val_accuracy: 0.7672\n",
            "Epoch 199/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5783 - accuracy: 0.7989 - val_loss: 1.0606 - val_accuracy: 0.7679\n",
            "Epoch 200/200\n",
            "2989/2989 [==============================] - 6s 2ms/step - loss: 0.5782 - accuracy: 0.7929 - val_loss: 1.0957 - val_accuracy: 0.7692\n",
            "Score for fold 2: loss of 1.0956845111073459; accuracy of 76.92307829856873%\n",
            "------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Train on 2990 samples, validate on 1494 samples\n",
            "Epoch 1/200\n",
            "2990/2990 [==============================] - 7s 3ms/step - loss: 1.8371 - accuracy: 0.3950 - val_loss: 1.7396 - val_accuracy: 0.5361\n",
            "Epoch 2/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.5799 - accuracy: 0.4579 - val_loss: 1.5355 - val_accuracy: 0.5556\n",
            "Epoch 3/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.5117 - accuracy: 0.4883 - val_loss: 1.3182 - val_accuracy: 0.5924\n",
            "Epoch 4/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.4124 - accuracy: 0.5174 - val_loss: 1.1636 - val_accuracy: 0.5991\n",
            "Epoch 5/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.3382 - accuracy: 0.5391 - val_loss: 1.1309 - val_accuracy: 0.6104\n",
            "Epoch 6/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.3180 - accuracy: 0.5455 - val_loss: 1.1157 - val_accuracy: 0.6084\n",
            "Epoch 7/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.2694 - accuracy: 0.5535 - val_loss: 1.0853 - val_accuracy: 0.6212\n",
            "Epoch 8/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.2364 - accuracy: 0.5846 - val_loss: 1.1239 - val_accuracy: 0.6178\n",
            "Epoch 9/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.2387 - accuracy: 0.5759 - val_loss: 1.0956 - val_accuracy: 0.6258\n",
            "Epoch 10/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.2047 - accuracy: 0.5886 - val_loss: 1.1025 - val_accuracy: 0.6299\n",
            "Epoch 11/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.1695 - accuracy: 0.5957 - val_loss: 1.1013 - val_accuracy: 0.6339\n",
            "Epoch 12/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.1795 - accuracy: 0.5893 - val_loss: 1.0486 - val_accuracy: 0.6432\n",
            "Epoch 13/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.1706 - accuracy: 0.5943 - val_loss: 1.0571 - val_accuracy: 0.6272\n",
            "Epoch 14/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.1486 - accuracy: 0.5940 - val_loss: 1.0505 - val_accuracy: 0.6372\n",
            "Epoch 15/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.1455 - accuracy: 0.5980 - val_loss: 1.1524 - val_accuracy: 0.6278\n",
            "Epoch 16/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.1080 - accuracy: 0.6057 - val_loss: 1.0774 - val_accuracy: 0.6506\n",
            "Epoch 17/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.1128 - accuracy: 0.6043 - val_loss: 1.0497 - val_accuracy: 0.6600\n",
            "Epoch 18/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.1265 - accuracy: 0.6020 - val_loss: 1.0662 - val_accuracy: 0.6379\n",
            "Epoch 19/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.1154 - accuracy: 0.6084 - val_loss: 1.0296 - val_accuracy: 0.6566\n",
            "Epoch 20/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.0706 - accuracy: 0.6137 - val_loss: 1.0265 - val_accuracy: 0.6459\n",
            "Epoch 21/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.0675 - accuracy: 0.6201 - val_loss: 1.0748 - val_accuracy: 0.6606\n",
            "Epoch 22/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.0412 - accuracy: 0.6271 - val_loss: 1.0456 - val_accuracy: 0.6720\n",
            "Epoch 23/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 1.0259 - accuracy: 0.6291 - val_loss: 1.0177 - val_accuracy: 0.6787\n",
            "Epoch 24/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9980 - accuracy: 0.6321 - val_loss: 1.0019 - val_accuracy: 0.6546\n",
            "Epoch 25/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9847 - accuracy: 0.6368 - val_loss: 1.0091 - val_accuracy: 0.6747\n",
            "Epoch 26/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9958 - accuracy: 0.6385 - val_loss: 1.0196 - val_accuracy: 0.6821\n",
            "Epoch 27/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9692 - accuracy: 0.6361 - val_loss: 1.0333 - val_accuracy: 0.6486\n",
            "Epoch 28/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9478 - accuracy: 0.6478 - val_loss: 1.0232 - val_accuracy: 0.6787\n",
            "Epoch 29/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9624 - accuracy: 0.6401 - val_loss: 1.0768 - val_accuracy: 0.6600\n",
            "Epoch 30/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9475 - accuracy: 0.6585 - val_loss: 1.0377 - val_accuracy: 0.6901\n",
            "Epoch 31/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9282 - accuracy: 0.6559 - val_loss: 1.0113 - val_accuracy: 0.6968\n",
            "Epoch 32/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9360 - accuracy: 0.6545 - val_loss: 1.0076 - val_accuracy: 0.6794\n",
            "Epoch 33/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9266 - accuracy: 0.6555 - val_loss: 1.0548 - val_accuracy: 0.7169\n",
            "Epoch 34/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9269 - accuracy: 0.6572 - val_loss: 1.1017 - val_accuracy: 0.7028\n",
            "Epoch 35/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8874 - accuracy: 0.6662 - val_loss: 1.0238 - val_accuracy: 0.6954\n",
            "Epoch 36/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.9042 - accuracy: 0.6659 - val_loss: 1.0160 - val_accuracy: 0.7041\n",
            "Epoch 37/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8936 - accuracy: 0.6682 - val_loss: 1.0512 - val_accuracy: 0.6941\n",
            "Epoch 38/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8952 - accuracy: 0.6742 - val_loss: 1.0519 - val_accuracy: 0.7095\n",
            "Epoch 39/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8709 - accuracy: 0.6843 - val_loss: 1.0923 - val_accuracy: 0.7129\n",
            "Epoch 40/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8667 - accuracy: 0.6696 - val_loss: 1.0871 - val_accuracy: 0.6948\n",
            "Epoch 41/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8651 - accuracy: 0.6736 - val_loss: 1.2026 - val_accuracy: 0.7015\n",
            "Epoch 42/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8608 - accuracy: 0.6846 - val_loss: 1.1539 - val_accuracy: 0.6901\n",
            "Epoch 43/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8490 - accuracy: 0.6793 - val_loss: 1.1064 - val_accuracy: 0.6995\n",
            "Epoch 44/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8371 - accuracy: 0.6826 - val_loss: 1.1694 - val_accuracy: 0.7008\n",
            "Epoch 45/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8562 - accuracy: 0.6786 - val_loss: 1.1345 - val_accuracy: 0.7216\n",
            "Epoch 46/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8418 - accuracy: 0.6796 - val_loss: 1.1930 - val_accuracy: 0.7102\n",
            "Epoch 47/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8423 - accuracy: 0.6843 - val_loss: 1.1893 - val_accuracy: 0.7129\n",
            "Epoch 48/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8419 - accuracy: 0.6776 - val_loss: 1.1658 - val_accuracy: 0.7162\n",
            "Epoch 49/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8140 - accuracy: 0.6913 - val_loss: 1.1662 - val_accuracy: 0.7349\n",
            "Epoch 50/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8102 - accuracy: 0.6983 - val_loss: 1.2492 - val_accuracy: 0.7216\n",
            "Epoch 51/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8308 - accuracy: 0.6973 - val_loss: 1.1914 - val_accuracy: 0.7256\n",
            "Epoch 52/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8071 - accuracy: 0.6970 - val_loss: 1.2734 - val_accuracy: 0.7229\n",
            "Epoch 53/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8052 - accuracy: 0.6936 - val_loss: 1.2982 - val_accuracy: 0.7182\n",
            "Epoch 54/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8069 - accuracy: 0.6923 - val_loss: 1.2444 - val_accuracy: 0.7309\n",
            "Epoch 55/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7909 - accuracy: 0.7040 - val_loss: 1.1672 - val_accuracy: 0.7410\n",
            "Epoch 56/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7755 - accuracy: 0.7114 - val_loss: 1.2409 - val_accuracy: 0.7396\n",
            "Epoch 57/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7970 - accuracy: 0.7064 - val_loss: 1.3525 - val_accuracy: 0.7289\n",
            "Epoch 58/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.8085 - accuracy: 0.6940 - val_loss: 1.2391 - val_accuracy: 0.7416\n",
            "Epoch 59/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7872 - accuracy: 0.7037 - val_loss: 1.3756 - val_accuracy: 0.7363\n",
            "Epoch 60/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7788 - accuracy: 0.7084 - val_loss: 1.1767 - val_accuracy: 0.7189\n",
            "Epoch 61/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7571 - accuracy: 0.7177 - val_loss: 1.3375 - val_accuracy: 0.7463\n",
            "Epoch 62/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7833 - accuracy: 0.7070 - val_loss: 1.2419 - val_accuracy: 0.7376\n",
            "Epoch 63/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7594 - accuracy: 0.7070 - val_loss: 1.3861 - val_accuracy: 0.7323\n",
            "Epoch 64/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7442 - accuracy: 0.7211 - val_loss: 1.3288 - val_accuracy: 0.7323\n",
            "Epoch 65/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7480 - accuracy: 0.7191 - val_loss: 1.2825 - val_accuracy: 0.7369\n",
            "Epoch 66/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7430 - accuracy: 0.7161 - val_loss: 1.3398 - val_accuracy: 0.7356\n",
            "Epoch 67/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7701 - accuracy: 0.7147 - val_loss: 1.1722 - val_accuracy: 0.7390\n",
            "Epoch 68/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7513 - accuracy: 0.7171 - val_loss: 1.4301 - val_accuracy: 0.7403\n",
            "Epoch 69/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7464 - accuracy: 0.7214 - val_loss: 1.3195 - val_accuracy: 0.7329\n",
            "Epoch 70/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7325 - accuracy: 0.7251 - val_loss: 1.2598 - val_accuracy: 0.7356\n",
            "Epoch 71/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7244 - accuracy: 0.7207 - val_loss: 1.3332 - val_accuracy: 0.7490\n",
            "Epoch 72/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7422 - accuracy: 0.7254 - val_loss: 1.3522 - val_accuracy: 0.7423\n",
            "Epoch 73/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7370 - accuracy: 0.7137 - val_loss: 1.3874 - val_accuracy: 0.7349\n",
            "Epoch 74/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7499 - accuracy: 0.7191 - val_loss: 1.2901 - val_accuracy: 0.7403\n",
            "Epoch 75/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7114 - accuracy: 0.7284 - val_loss: 1.3953 - val_accuracy: 0.7430\n",
            "Epoch 76/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7282 - accuracy: 0.7237 - val_loss: 1.4008 - val_accuracy: 0.7483\n",
            "Epoch 77/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7065 - accuracy: 0.7291 - val_loss: 1.5885 - val_accuracy: 0.7416\n",
            "Epoch 78/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7168 - accuracy: 0.7294 - val_loss: 1.5730 - val_accuracy: 0.7483\n",
            "Epoch 79/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7123 - accuracy: 0.7308 - val_loss: 1.4723 - val_accuracy: 0.7256\n",
            "Epoch 80/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7051 - accuracy: 0.7291 - val_loss: 1.3182 - val_accuracy: 0.7530\n",
            "Epoch 81/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7230 - accuracy: 0.7231 - val_loss: 1.4577 - val_accuracy: 0.7410\n",
            "Epoch 82/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6837 - accuracy: 0.7405 - val_loss: 1.4447 - val_accuracy: 0.7490\n",
            "Epoch 83/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6974 - accuracy: 0.7381 - val_loss: 1.4460 - val_accuracy: 0.7517\n",
            "Epoch 84/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6901 - accuracy: 0.7304 - val_loss: 1.5153 - val_accuracy: 0.7483\n",
            "Epoch 85/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6703 - accuracy: 0.7522 - val_loss: 1.4680 - val_accuracy: 0.7523\n",
            "Epoch 86/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.7070 - accuracy: 0.7304 - val_loss: 1.5239 - val_accuracy: 0.7430\n",
            "Epoch 87/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6787 - accuracy: 0.7411 - val_loss: 1.6483 - val_accuracy: 0.7483\n",
            "Epoch 88/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6731 - accuracy: 0.7488 - val_loss: 1.7708 - val_accuracy: 0.7410\n",
            "Epoch 89/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6934 - accuracy: 0.7324 - val_loss: 1.3692 - val_accuracy: 0.7450\n",
            "Epoch 90/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6800 - accuracy: 0.7418 - val_loss: 1.4310 - val_accuracy: 0.7510\n",
            "Epoch 91/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6699 - accuracy: 0.7415 - val_loss: 1.4774 - val_accuracy: 0.7604\n",
            "Epoch 92/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6835 - accuracy: 0.7395 - val_loss: 1.5433 - val_accuracy: 0.7537\n",
            "Epoch 93/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6813 - accuracy: 0.7458 - val_loss: 1.4217 - val_accuracy: 0.7617\n",
            "Epoch 94/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6564 - accuracy: 0.7525 - val_loss: 1.4196 - val_accuracy: 0.7584\n",
            "Epoch 95/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6645 - accuracy: 0.7398 - val_loss: 1.4311 - val_accuracy: 0.7671\n",
            "Epoch 96/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6734 - accuracy: 0.7428 - val_loss: 1.4950 - val_accuracy: 0.7550\n",
            "Epoch 97/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6748 - accuracy: 0.7438 - val_loss: 1.6052 - val_accuracy: 0.7704\n",
            "Epoch 98/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6718 - accuracy: 0.7452 - val_loss: 1.4665 - val_accuracy: 0.7677\n",
            "Epoch 99/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6364 - accuracy: 0.7522 - val_loss: 1.5346 - val_accuracy: 0.7470\n",
            "Epoch 100/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6763 - accuracy: 0.7532 - val_loss: 1.4267 - val_accuracy: 0.7610\n",
            "Epoch 101/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6580 - accuracy: 0.7565 - val_loss: 1.5091 - val_accuracy: 0.7590\n",
            "Epoch 102/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6378 - accuracy: 0.7495 - val_loss: 1.4582 - val_accuracy: 0.7751\n",
            "Epoch 103/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6440 - accuracy: 0.7535 - val_loss: 1.5207 - val_accuracy: 0.7711\n",
            "Epoch 104/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6325 - accuracy: 0.7672 - val_loss: 1.4844 - val_accuracy: 0.7684\n",
            "Epoch 105/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6490 - accuracy: 0.7472 - val_loss: 1.5129 - val_accuracy: 0.7644\n",
            "Epoch 106/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6506 - accuracy: 0.7542 - val_loss: 1.6813 - val_accuracy: 0.7771\n",
            "Epoch 107/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6346 - accuracy: 0.7656 - val_loss: 1.6070 - val_accuracy: 0.7637\n",
            "Epoch 108/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6313 - accuracy: 0.7582 - val_loss: 1.7558 - val_accuracy: 0.7778\n",
            "Epoch 109/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6506 - accuracy: 0.7475 - val_loss: 1.6352 - val_accuracy: 0.7644\n",
            "Epoch 110/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6245 - accuracy: 0.7649 - val_loss: 1.8241 - val_accuracy: 0.7631\n",
            "Epoch 111/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6270 - accuracy: 0.7622 - val_loss: 1.7766 - val_accuracy: 0.7657\n",
            "Epoch 112/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6470 - accuracy: 0.7542 - val_loss: 1.7127 - val_accuracy: 0.7664\n",
            "Epoch 113/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6268 - accuracy: 0.7635 - val_loss: 1.9439 - val_accuracy: 0.7617\n",
            "Epoch 114/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6225 - accuracy: 0.7629 - val_loss: 1.8506 - val_accuracy: 0.7811\n",
            "Epoch 115/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6245 - accuracy: 0.7635 - val_loss: 2.0294 - val_accuracy: 0.7731\n",
            "Epoch 116/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6146 - accuracy: 0.7656 - val_loss: 1.7568 - val_accuracy: 0.7691\n",
            "Epoch 117/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6133 - accuracy: 0.7682 - val_loss: 1.8227 - val_accuracy: 0.7818\n",
            "Epoch 118/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6125 - accuracy: 0.7702 - val_loss: 1.6309 - val_accuracy: 0.7731\n",
            "Epoch 119/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5911 - accuracy: 0.7936 - val_loss: 1.6657 - val_accuracy: 0.7610\n",
            "Epoch 120/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6171 - accuracy: 0.7639 - val_loss: 1.6468 - val_accuracy: 0.7604\n",
            "Epoch 121/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6133 - accuracy: 0.7652 - val_loss: 1.4835 - val_accuracy: 0.7657\n",
            "Epoch 122/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6037 - accuracy: 0.7766 - val_loss: 1.5309 - val_accuracy: 0.7738\n",
            "Epoch 123/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5993 - accuracy: 0.7776 - val_loss: 1.6654 - val_accuracy: 0.7751\n",
            "Epoch 124/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6056 - accuracy: 0.7682 - val_loss: 1.8094 - val_accuracy: 0.7677\n",
            "Epoch 125/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6033 - accuracy: 0.7689 - val_loss: 1.6572 - val_accuracy: 0.7711\n",
            "Epoch 126/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5831 - accuracy: 0.7719 - val_loss: 1.7373 - val_accuracy: 0.7751\n",
            "Epoch 127/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.6045 - accuracy: 0.7662 - val_loss: 1.8083 - val_accuracy: 0.7851\n",
            "Epoch 128/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5706 - accuracy: 0.7803 - val_loss: 1.6993 - val_accuracy: 0.7878\n",
            "Epoch 129/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5776 - accuracy: 0.7779 - val_loss: 1.8484 - val_accuracy: 0.7851\n",
            "Epoch 130/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5921 - accuracy: 0.7656 - val_loss: 1.7852 - val_accuracy: 0.7798\n",
            "Epoch 131/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5980 - accuracy: 0.7776 - val_loss: 1.8796 - val_accuracy: 0.7959\n",
            "Epoch 132/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5902 - accuracy: 0.7763 - val_loss: 1.7469 - val_accuracy: 0.7825\n",
            "Epoch 133/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5670 - accuracy: 0.7853 - val_loss: 1.8230 - val_accuracy: 0.7718\n",
            "Epoch 134/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5818 - accuracy: 0.7749 - val_loss: 1.7603 - val_accuracy: 0.7744\n",
            "Epoch 135/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5989 - accuracy: 0.7742 - val_loss: 1.6303 - val_accuracy: 0.7858\n",
            "Epoch 136/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5923 - accuracy: 0.7766 - val_loss: 1.7059 - val_accuracy: 0.7912\n",
            "Epoch 137/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5625 - accuracy: 0.7736 - val_loss: 1.6806 - val_accuracy: 0.7831\n",
            "Epoch 138/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5843 - accuracy: 0.7826 - val_loss: 1.5457 - val_accuracy: 0.7771\n",
            "Epoch 139/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5775 - accuracy: 0.7786 - val_loss: 1.7958 - val_accuracy: 0.7791\n",
            "Epoch 140/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5695 - accuracy: 0.7709 - val_loss: 1.5901 - val_accuracy: 0.7684\n",
            "Epoch 141/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5572 - accuracy: 0.7860 - val_loss: 1.6904 - val_accuracy: 0.7932\n",
            "Epoch 142/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5734 - accuracy: 0.7890 - val_loss: 1.7571 - val_accuracy: 0.7925\n",
            "Epoch 143/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5753 - accuracy: 0.7809 - val_loss: 1.9284 - val_accuracy: 0.7918\n",
            "Epoch 144/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5682 - accuracy: 0.7679 - val_loss: 2.1075 - val_accuracy: 0.7952\n",
            "Epoch 145/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5802 - accuracy: 0.7729 - val_loss: 1.8369 - val_accuracy: 0.7845\n",
            "Epoch 146/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5696 - accuracy: 0.7836 - val_loss: 2.0128 - val_accuracy: 0.7878\n",
            "Epoch 147/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5872 - accuracy: 0.7816 - val_loss: 1.6594 - val_accuracy: 0.7758\n",
            "Epoch 148/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5730 - accuracy: 0.7786 - val_loss: 1.8847 - val_accuracy: 0.7825\n",
            "Epoch 149/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5721 - accuracy: 0.7796 - val_loss: 1.8951 - val_accuracy: 0.7751\n",
            "Epoch 150/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5501 - accuracy: 0.7876 - val_loss: 1.8000 - val_accuracy: 0.7845\n",
            "Epoch 151/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5904 - accuracy: 0.7786 - val_loss: 1.9103 - val_accuracy: 0.7851\n",
            "Epoch 152/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5919 - accuracy: 0.7793 - val_loss: 2.0149 - val_accuracy: 0.7664\n",
            "Epoch 153/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5450 - accuracy: 0.7823 - val_loss: 1.9931 - val_accuracy: 0.7851\n",
            "Epoch 154/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5727 - accuracy: 0.7686 - val_loss: 1.8208 - val_accuracy: 0.7965\n",
            "Epoch 155/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5695 - accuracy: 0.7886 - val_loss: 2.1219 - val_accuracy: 0.7851\n",
            "Epoch 156/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5572 - accuracy: 0.7890 - val_loss: 1.7347 - val_accuracy: 0.7871\n",
            "Epoch 157/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5502 - accuracy: 0.7816 - val_loss: 1.7204 - val_accuracy: 0.7979\n",
            "Epoch 158/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5705 - accuracy: 0.7823 - val_loss: 1.6423 - val_accuracy: 0.7918\n",
            "Epoch 159/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5581 - accuracy: 0.7813 - val_loss: 1.7259 - val_accuracy: 0.7898\n",
            "Epoch 160/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5428 - accuracy: 0.7849 - val_loss: 2.0401 - val_accuracy: 0.7858\n",
            "Epoch 161/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5598 - accuracy: 0.7893 - val_loss: 1.7754 - val_accuracy: 0.7825\n",
            "Epoch 162/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5455 - accuracy: 0.7873 - val_loss: 1.7374 - val_accuracy: 0.7918\n",
            "Epoch 163/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5452 - accuracy: 0.7886 - val_loss: 1.8916 - val_accuracy: 0.7925\n",
            "Epoch 164/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5416 - accuracy: 0.7973 - val_loss: 2.1045 - val_accuracy: 0.7865\n",
            "Epoch 165/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5571 - accuracy: 0.7876 - val_loss: 2.3096 - val_accuracy: 0.7885\n",
            "Epoch 166/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5502 - accuracy: 0.7906 - val_loss: 2.0820 - val_accuracy: 0.7805\n",
            "Epoch 167/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5435 - accuracy: 0.7890 - val_loss: 2.0034 - val_accuracy: 0.7791\n",
            "Epoch 168/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5348 - accuracy: 0.7936 - val_loss: 1.9272 - val_accuracy: 0.7811\n",
            "Epoch 169/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5444 - accuracy: 0.7893 - val_loss: 1.8611 - val_accuracy: 0.7845\n",
            "Epoch 170/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5435 - accuracy: 0.8010 - val_loss: 2.0944 - val_accuracy: 0.7851\n",
            "Epoch 171/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5332 - accuracy: 0.7936 - val_loss: 1.9318 - val_accuracy: 0.7932\n",
            "Epoch 172/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5680 - accuracy: 0.7826 - val_loss: 1.8701 - val_accuracy: 0.7865\n",
            "Epoch 173/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5249 - accuracy: 0.7883 - val_loss: 1.8341 - val_accuracy: 0.7992\n",
            "Epoch 174/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5299 - accuracy: 0.7987 - val_loss: 1.7551 - val_accuracy: 0.7972\n",
            "Epoch 175/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5198 - accuracy: 0.8037 - val_loss: 2.2170 - val_accuracy: 0.7851\n",
            "Epoch 176/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5269 - accuracy: 0.8010 - val_loss: 2.0467 - val_accuracy: 0.7925\n",
            "Epoch 177/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5388 - accuracy: 0.7993 - val_loss: 1.7529 - val_accuracy: 0.7878\n",
            "Epoch 178/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5164 - accuracy: 0.8104 - val_loss: 2.0164 - val_accuracy: 0.7945\n",
            "Epoch 179/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5357 - accuracy: 0.7943 - val_loss: 1.8237 - val_accuracy: 0.8012\n",
            "Epoch 180/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5101 - accuracy: 0.8054 - val_loss: 1.8859 - val_accuracy: 0.7878\n",
            "Epoch 181/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5333 - accuracy: 0.7953 - val_loss: 2.1343 - val_accuracy: 0.7992\n",
            "Epoch 182/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5326 - accuracy: 0.7936 - val_loss: 1.9437 - val_accuracy: 0.7992\n",
            "Epoch 183/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5479 - accuracy: 0.7916 - val_loss: 1.9991 - val_accuracy: 0.7925\n",
            "Epoch 184/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5320 - accuracy: 0.7960 - val_loss: 1.7846 - val_accuracy: 0.8039\n",
            "Epoch 185/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5099 - accuracy: 0.8140 - val_loss: 2.0197 - val_accuracy: 0.7965\n",
            "Epoch 186/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5390 - accuracy: 0.7983 - val_loss: 2.0568 - val_accuracy: 0.7985\n",
            "Epoch 187/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5072 - accuracy: 0.8090 - val_loss: 2.0252 - val_accuracy: 0.8012\n",
            "Epoch 188/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5061 - accuracy: 0.8114 - val_loss: 2.3974 - val_accuracy: 0.7945\n",
            "Epoch 189/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5370 - accuracy: 0.8013 - val_loss: 1.8940 - val_accuracy: 0.7918\n",
            "Epoch 190/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5243 - accuracy: 0.8050 - val_loss: 2.5312 - val_accuracy: 0.7959\n",
            "Epoch 191/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5159 - accuracy: 0.8027 - val_loss: 2.0236 - val_accuracy: 0.7858\n",
            "Epoch 192/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.4986 - accuracy: 0.8124 - val_loss: 2.0928 - val_accuracy: 0.7999\n",
            "Epoch 193/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5334 - accuracy: 0.8030 - val_loss: 1.7775 - val_accuracy: 0.7878\n",
            "Epoch 194/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5140 - accuracy: 0.8057 - val_loss: 1.8657 - val_accuracy: 0.7972\n",
            "Epoch 195/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5182 - accuracy: 0.8120 - val_loss: 2.2611 - val_accuracy: 0.7979\n",
            "Epoch 196/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5113 - accuracy: 0.8040 - val_loss: 2.0802 - val_accuracy: 0.7965\n",
            "Epoch 197/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5052 - accuracy: 0.8130 - val_loss: 1.8368 - val_accuracy: 0.7985\n",
            "Epoch 198/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5219 - accuracy: 0.7977 - val_loss: 2.3863 - val_accuracy: 0.7985\n",
            "Epoch 199/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5080 - accuracy: 0.8067 - val_loss: 1.7300 - val_accuracy: 0.7945\n",
            "Epoch 200/200\n",
            "2990/2990 [==============================] - 6s 2ms/step - loss: 0.5150 - accuracy: 0.8090 - val_loss: 1.9608 - val_accuracy: 0.7979\n",
            "Score for fold 3: loss of 1.9608078734223622; accuracy of 79.78581190109253%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f1V3XzqXDyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "d7950f60-cefa-456e-8914-35e9506ac864"
      },
      "source": [
        "print('------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('-----------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('--------------------------------------------------')\n",
        "print('Average score for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)}')\n",
        "print(f'>Loss: {np.mean(loss_per_fold)}')\n",
        "#print(model.summary())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------\n",
            "Score per fold\n",
            "-----------------------------------------------------\n",
            "> Fold 1 - Loss: 1.1561970226342064 - Accuracy: 79.3311059474945%\n",
            "-----------------------------------------------------\n",
            "> Fold 2 - Loss: 1.0956845111073459 - Accuracy: 76.92307829856873%\n",
            "-----------------------------------------------------\n",
            "> Fold 3 - Loss: 1.9608078734223622 - Accuracy: 79.78581190109253%\n",
            "--------------------------------------------------\n",
            "Average score for all folds:\n",
            "> Accuracy: 78.67999871571858\n",
            ">Loss: 1.4042298023879713\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}